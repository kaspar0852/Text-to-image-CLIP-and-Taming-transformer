{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjZ7lcrTpANAAU09Dnr7Dd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaspar0852/Text-to-image-CLIP-and-Taming-transformer/blob/main/Text_To_Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuF-ZRO4AGFt"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/CompVis/taming-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## install some libraries\n",
        "!pip install --no-deps ftfy regex tqdm\n",
        "!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "!pip uninstall torchtext --yes\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "qSF7vcLFAWdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import torch,os,imageio,pdb,math\n",
        "import torchvision \n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import PIL \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarning('ignore')"
      ],
      "metadata": {
        "id": "EHsVJq-5BUcE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##helper functions\n",
        "\n",
        "def show_from_tensor(tensor):\n",
        "  img = tensor.clone()\n",
        "  img = img.mul(255).byte()\n",
        "  img = img.cpu().numpy().transpose((1,2,0))\n",
        "\n",
        "  plt.figure(figsize = (10,7))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "  #this function will take tensor and display it as an image\n",
        "\n",
        "def norm_data(data):\n",
        "   #it will receive the data and normalize it in a follwoing way\n",
        "   return(data.clip(-1,1)+1)/2 ##we are going to move in between 0 to 1 in result\n",
        "\n",
        "##Parameters and Hyperparameters\n",
        "learning_rate = .5\n",
        "batch_size = 1\n",
        "wd = .1 #weight decay is a regulizer paramerter that is going to help the optimizer limit the size-\n",
        "#-of weights to improve the generalization capabilities of the architecture\n",
        "noise_factor = .1 \n",
        "\n",
        "total_iter = 100 #increace this for more refined results\n",
        "#as we take a text prompt and we begin to generate the image,then there will be comparison of encoding of text prompt-\n",
        "#-with the enoding of the image we are generating and we calculate the loss suing the Cosine Similarity-\n",
        "#-mathematical function applied to the result of those encodings\n",
        "\n",
        "im_shape = [225,400,3] #height,width ,channel\n",
        "size1, size2,channels = im_shape\n"
      ],
      "metadata": {
        "id": "VlrABpGsCJtV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up the CLIP model\n",
        "clipmodel, _ = clip.load('ViT-B/32',jit = False)\n",
        "clipmodel.eval()\n",
        "print(clip.available_models())\n",
        "\n",
        "print('Clip model visual input resolution:',clipmodel.visual.input_resolution)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache()\n",
        "#We can proceed to use clip in inferrence, eval mode to encode texts and images directly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkhiTEauFGN1",
        "outputId": "28aa456f-fd8f-4e65-a044-b3d9bbd5d89f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 102MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
            "Clip model visual input resolution: 224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Taming transformer instantiation\n",
        "\n",
        "%cd taming-transformers/\n",
        "\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/configs\n",
        "\n",
        "if len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt' \n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml' \n"
      ],
      "metadata": {
        "id": "fgpI5yWoFvSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from taming.models.vqgan import VQModel\n",
        "\n",
        "#we need a couple of helper functions\n",
        "def load_config(config_path,display = False):\n",
        "  config_data = OmegaConf.load(config_path)\n",
        "  if display:\n",
        "    print(yaml.dump(OmegaConf.to_container(config_data)))\n",
        "  return config_data\n",
        "\n",
        "def load_vqgan(config,chk_path = None):\n",
        "  model = VQModel(**config.model.params)\n",
        "  if chk_path is not None:\n",
        "    state_dict = torch.load(chk_path, map_location =\"cpu\")[\"state_dict\"]\n",
        "    missing,unexpected = model.load_state_dict(state_dict,strict = False)\n",
        "  return model.eval()\n",
        "\n",
        "def generator(x):\n",
        "  x = taming_model.post_quant_conv(x)\n",
        "  x = taming_model.decoder(x)\n",
        "  return x \n",
        "  #when we generate something from this model,it has to go through two stages-\n",
        "  #-1 is the convolutional network and 2nd actual decoder of the transformer\n",
        "\n",
        "taming_config = load_config(\"./models/vqgan_imagenet_f16_16384/configs/model.yaml\",display = True)\n",
        "taming_model = load_vqgan(taming_config,chk_path = \"./models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(device)\n"
      ],
      "metadata": {
        "id": "1AeBJgqNJBA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are seeing all the detailed information of the generator /transformer model when we execute the -\n",
        "#-top code it uses 256 channels,and patches of 16 * 16 pixels each"
      ],
      "metadata": {
        "id": "XNHgNGtqOUBa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ag_KAAWgOzKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}